{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4991e165",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "TRAIN_PATH = '../data/train_clean.csv'\n",
    "TEST_PATH = '../data/test_clean.csv'\n",
    "\n",
    "# Cargar datasets\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Comprobaciones \n",
    "print(\"-Archivos cargados-\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test  shape: {test.shape}\")\n",
    "\n",
    "print(\"\\n--- Primeras 5 filas de train ---\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\n--- Tipos de columnas (train) ---\")\n",
    "print(train.dtypes.value_counts())\n",
    "\n",
    "# Distribucion de la variable objetivo en train\n",
    "if 'label' in train.columns:\n",
    "    vc = train['label'].value_counts(dropna=False).sort_index()\n",
    "    pct = train['label'].value_counts(normalize=True, dropna=False).sort_index() * 100\n",
    "    dist_df = pd.DataFrame({'count': vc, 'pct': pct.round(4)})\n",
    "    print(\"\\n--- Distribución 'label' (train) ---\")\n",
    "    print(dist_df)\n",
    "else:\n",
    "    print(\"La columna 'label' NO se encontró en train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = train[train['label'].isin([0, 1])].copy()\n",
    "\n",
    "FEATURES = [\n",
    "    'age_range', 'gender', 'merchant_id', 'activity_len', 'actions_0',\n",
    "    'actions_2', 'actions_3', 'unique_items', 'unique_categories',\n",
    "    'unique_brands', 'day_span', 'has_1111'\n",
    "]\n",
    "X = df_model[FEATURES]\n",
    "y = df_model['label'].astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f078e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.62887\teval-auc:0.61302\n",
      "[10]\ttrain-auc:0.64052\teval-auc:0.62505\n",
      "[20]\ttrain-auc:0.64482\teval-auc:0.62693\n",
      "[30]\ttrain-auc:0.64959\teval-auc:0.62843\n",
      "[40]\ttrain-auc:0.65441\teval-auc:0.63060\n",
      "[50]\ttrain-auc:0.65942\teval-auc:0.63220\n",
      "[60]\ttrain-auc:0.66324\teval-auc:0.63318\n",
      "[70]\ttrain-auc:0.66870\teval-auc:0.63517\n",
      "[80]\ttrain-auc:0.67376\teval-auc:0.63780\n",
      "[90]\ttrain-auc:0.67773\teval-auc:0.63952\n",
      "[100]\ttrain-auc:0.68035\teval-auc:0.64001\n",
      "[110]\ttrain-auc:0.68331\teval-auc:0.64067\n",
      "[120]\ttrain-auc:0.68562\teval-auc:0.64094\n",
      "[130]\ttrain-auc:0.68866\teval-auc:0.64182\n",
      "[140]\ttrain-auc:0.69117\teval-auc:0.64158\n",
      "[150]\ttrain-auc:0.69443\teval-auc:0.64231\n",
      "[160]\ttrain-auc:0.69689\teval-auc:0.64291\n",
      "[170]\ttrain-auc:0.70028\teval-auc:0.64352\n",
      "[180]\ttrain-auc:0.70285\teval-auc:0.64373\n",
      "[190]\ttrain-auc:0.70442\teval-auc:0.64356\n",
      "[200]\ttrain-auc:0.70611\teval-auc:0.64305\n",
      "[210]\ttrain-auc:0.70904\teval-auc:0.64292\n",
      "[220]\ttrain-auc:0.71125\teval-auc:0.64262\n",
      "[225]\ttrain-auc:0.71203\teval-auc:0.64253\n",
      "\n",
      "--- Métricas en validacion ---\n",
      "AUC (val): 0.6425714519214235\n",
      "\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9560    0.7190    0.8208     48983\n",
      "           1     0.1023    0.4915    0.1693      3190\n",
      "\n",
      "    accuracy                         0.7051     52173\n",
      "   macro avg     0.5291    0.6053    0.4950     52173\n",
      "weighted avg     0.9038    0.7051    0.7809     52173\n",
      "\n",
      "\n",
      "Confusion matrix (val):\n",
      "[[35221 13762]\n",
      " [ 1622  1568]]\n",
      "\n",
      "Top 20 features (gain):\n",
      "unique_items         -> gain: 111.427284\n",
      "actions_2            -> gain: 92.481766\n",
      "unique_categories    -> gain: 85.459549\n",
      "merchant_id          -> gain: 47.667240\n",
      "day_span             -> gain: 40.220005\n",
      "gender               -> gain: 33.075409\n",
      "unique_brands        -> gain: 31.978071\n",
      "actions_0            -> gain: 30.313925\n",
      "age_range            -> gain: 30.290596\n",
      "activity_len         -> gain: 29.675341\n",
      "actions_3            -> gain: 24.154499\n",
      "has_1111             -> gain: 9.871352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convertir a DMatrix (estructura de datos interna optimizada de XGBoost para almacenar features y etiquetas.)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val)\n",
    "\n",
    "# Parametros\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'eta': 0.05,            # learning_rate\n",
    "    'max_depth': 6,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'verbosity': 1,\n",
    "    'tree_method': 'hist'   # más eficiente; cambia a 'auto' si prefieres\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "# Entrenar con early stopping (num_boost_round = n_estimators original)\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=10\n",
    ")\n",
    "\n",
    "# Predicciones de probabilidad en validacion\n",
    "y_val_proba = bst.predict(dval)\n",
    "y_val_pred = (y_val_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metricas\n",
    "print(\"\\n--- Métricas en validacion ---\")\n",
    "print(\"AUC (val):\", roc_auc_score(y_val, y_val_proba))\n",
    "print(\"\\nClassification report (val):\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "print(\"\\nConfusion matrix (val):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "fi = bst.get_score(importance_type='gain')\n",
    "fi_sorted = sorted(fi.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 features (gain):\")\n",
    "for f, v in fi_sorted[:20]:\n",
    "    print(f\"{f:20s} -> gain: {v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56305ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (PR-AUC): 0.11883\n",
      "Mejor threshold por F1: 0.5827  -> F1: 0.1832\n",
      "Precision@best: 0.1319, Recall@best: 0.2997\n",
      "\n",
      "Classification report (con threshold optimo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9503    0.8715    0.9092     48983\n",
      "           1     0.1319    0.2997    0.1832      3190\n",
      "\n",
      "    accuracy                         0.8366     52173\n",
      "   macro avg     0.5411    0.5856    0.5462     52173\n",
      "weighted avg     0.9002    0.8366    0.8648     52173\n",
      "\n",
      "\n",
      "Confusion matrix (con threshold optimo):\n",
      "[[42691  6292]\n",
      " [ 2234   956]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1) PR-AUC (average precision)\n",
    "ap = average_precision_score(y_val, y_val_proba)\n",
    "print(\"Average Precision (PR-AUC):\", round(ap, 6))\n",
    "\n",
    "# 2) Encontrar umbral que maximice F1 en validacion\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Mejor threshold por F1: {best_threshold:.4f}  -> F1: {best_f1:.4f}\")\n",
    "print(f\"Precision@best: {precisions[:-1][best_idx]:.4f}, Recall@best: {recalls[:-1][best_idx]:.4f}\")\n",
    "\n",
    "# 3) Evaluar con ese umbral\n",
    "y_val_pred_best = (y_val_proba >= best_threshold).astype(int)\n",
    "print(\"\\nClassification report (con threshold optimo):\")\n",
    "print(classification_report(y_val, y_val_pred_best, digits=4))\n",
    "print(\"\\nConfusion matrix (con threshold optimo):\")\n",
    "print(confusion_matrix(y_val, y_val_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761a5d9",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899c1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X_train_lgb: (208691, 12) X_val_lgb: (52173, 12)\n",
      "scale_pos_weight (LGB): 15.352530951261558\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's average_precision: 0.117801\tvalid's average_precision: 0.11549\n",
      "[20]\ttrain's average_precision: 0.120762\tvalid's average_precision: 0.117246\n",
      "[30]\ttrain's average_precision: 0.122879\tvalid's average_precision: 0.117603\n",
      "[40]\ttrain's average_precision: 0.125651\tvalid's average_precision: 0.118473\n",
      "[50]\ttrain's average_precision: 0.128153\tvalid's average_precision: 0.119987\n",
      "[60]\ttrain's average_precision: 0.131758\tvalid's average_precision: 0.119949\n",
      "[70]\ttrain's average_precision: 0.134135\tvalid's average_precision: 0.119607\n",
      "[80]\ttrain's average_precision: 0.138021\tvalid's average_precision: 0.121288\n",
      "[90]\ttrain's average_precision: 0.141099\tvalid's average_precision: 0.122117\n",
      "[100]\ttrain's average_precision: 0.144531\tvalid's average_precision: 0.122797\n",
      "[110]\ttrain's average_precision: 0.14735\tvalid's average_precision: 0.123027\n",
      "[120]\ttrain's average_precision: 0.149918\tvalid's average_precision: 0.122921\n",
      "[130]\ttrain's average_precision: 0.152547\tvalid's average_precision: 0.122913\n",
      "[140]\ttrain's average_precision: 0.154595\tvalid's average_precision: 0.122714\n",
      "[150]\ttrain's average_precision: 0.157009\tvalid's average_precision: 0.122605\n",
      "[160]\ttrain's average_precision: 0.159571\tvalid's average_precision: 0.122472\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttrain's average_precision: 0.147971\tvalid's average_precision: 0.123226\n",
      "\n",
      "LGB PR-AUC (Average Precision): 0.123226\n",
      "Mejor threshold LGB por F1: 0.6053 -> F1: 0.1875\n",
      "\n",
      "Classification report (LGB, threshold optimo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9502    0.8826    0.9152     48983\n",
      "           1     0.1386    0.2900    0.1875      3190\n",
      "\n",
      "    accuracy                         0.8464     52173\n",
      "   macro avg     0.5444    0.5863    0.5513     52173\n",
      "weighted avg     0.9006    0.8464    0.8707     52173\n",
      "\n",
      "\n",
      "Confusion matrix (LGB, threshold optimo):\n",
      "[[43233  5750]\n",
      " [ 2265   925]]\n",
      "\n",
      "Top 20 features (gain) LGB:\n",
      "merchant_freq        -> gain: 116165.279728\n",
      "unique_items         -> gain: 95033.639186\n",
      "actions_2            -> gain: 42808.822344\n",
      "day_span             -> gain: 38091.357589\n",
      "unique_categories    -> gain: 36050.078897\n",
      "actions_0            -> gain: 17995.844117\n",
      "age_range            -> gain: 13992.625535\n",
      "gender               -> gain: 8517.452394\n",
      "activity_len         -> gain: 7094.600403\n",
      "actions_3            -> gain: 5781.873428\n",
      "unique_brands        -> gain: 5253.487522\n",
      "has_1111             -> gain: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "df_model = train[train['label'].isin([0,1])].copy()  \n",
    "\n",
    "\n",
    "merchant_counts = df_model['merchant_id'].value_counts()\n",
    "df_model['merchant_freq'] = df_model['merchant_id'].map(merchant_counts)\n",
    "\n",
    "\n",
    "features_lgb = [\n",
    "    'activity_len','actions_0','actions_2','actions_3',\n",
    "    'unique_items','unique_categories','unique_brands',\n",
    "    'day_span','has_1111','age_range','gender','merchant_freq'\n",
    "]\n",
    "\n",
    "X = df_model[features_lgb]\n",
    "y = df_model['label'].astype(int)\n",
    "\n",
    "\n",
    "X_train_lgb, X_val_lgb, y_train_lgb, y_val_lgb = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "\n",
    "scale_pos_weight_lgb = (y_train_lgb==0).sum() / (y_train_lgb==1).sum()\n",
    "print(\"Shapes -> X_train_lgb:\", X_train_lgb.shape, \"X_val_lgb:\", X_val_lgb.shape)\n",
    "print(\"scale_pos_weight (LGB):\", scale_pos_weight_lgb)\n",
    "\n",
    "# ---------- Dataset LightGBM ----------\n",
    "lgb_train = lgb.Dataset(X_train_lgb, label=y_train_lgb)\n",
    "lgb_val   = lgb.Dataset(X_val_lgb, label=y_val_lgb, reference=lgb_train)\n",
    "\n",
    "# ---------- Parametros  ----------\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'average_precision',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 6,\n",
    "    'verbosity': -1,\n",
    "    'is_unbalance': False,          \n",
    "    'scale_pos_weight': scale_pos_weight_lgb\n",
    "}\n",
    "\n",
    "\n",
    "gbm = lgb.train(\n",
    "    lgb_params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train','valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=10)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predicciones y evaluacion \n",
    "y_val_proba_lgb = gbm.predict(X_val_lgb)\n",
    "from sklearn.metrics import average_precision_score, classification_report, confusion_matrix\n",
    "ap_lgb = average_precision_score(y_val_lgb, y_val_proba_lgb)\n",
    "print(\"\\nLGB PR-AUC (Average Precision):\", round(ap_lgb, 6))\n",
    "\n",
    "# Buscar threshold que maximice F1\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_lgb, y_val_proba_lgb)\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold_lgb = thresholds[best_idx]\n",
    "print(f\"Mejor threshold LGB por F1: {best_threshold_lgb:.4f} -> F1: {f1_scores[best_idx]:.4f}\")\n",
    "y_val_pred_lgb = (y_val_proba_lgb >= best_threshold_lgb).astype(int)\n",
    "\n",
    "print(\"\\nClassification report (LGB, threshold optimo):\")\n",
    "print(classification_report(y_val_lgb, y_val_pred_lgb, digits=4))\n",
    "print(\"\\nConfusion matrix (LGB, threshold optimo):\")\n",
    "print(confusion_matrix(y_val_lgb, y_val_pred_lgb))\n",
    "\n",
    "# Top features (gain/importance)\n",
    "imp = gbm.feature_importance(importance_type='gain')\n",
    "names = gbm.feature_name()\n",
    "feat_imp = sorted(zip(names, imp), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 20 features (gain) LGB:\")\n",
    "for n, v in feat_imp[:20]:\n",
    "    print(f\"{n:20s} -> gain: {v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872be7c",
   "metadata": {},
   "source": [
    "## Análisis RFM Combinado con Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- HEAD  -----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>label</th>\n",
       "      <th>date_max</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>recency_score</th>\n",
       "      <th>frequency_score</th>\n",
       "      <th>purchases_score</th>\n",
       "      <th>monetary_score</th>\n",
       "      <th>RFM_score</th>\n",
       "      <th>multiple_dates</th>\n",
       "      <th>interacted_1111</th>\n",
       "      <th>rfm_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34176</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>VIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>34176</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>34176</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>VIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>34176</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Oportunista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>230784</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>362112</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Oportunista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  label   date_max  recency_days  recency_score  frequency_score  \\\n",
       "6      34176      0 2014-11-11             0              5                5   \n",
       "30     34176      0 2014-11-11             0              5                4   \n",
       "40     34176      1 2014-11-11             0              5                5   \n",
       "63     34176      0 2014-11-11             0              5                1   \n",
       "117   230784      0 2014-11-11             0              5                4   \n",
       "133   362112      0 2014-11-11             0              5                1   \n",
       "\n",
       "     purchases_score  monetary_score  RFM_score  multiple_dates  \\\n",
       "6                  5               5       5.00               1   \n",
       "30                 1               1       3.75               1   \n",
       "40                 1               3       4.50               1   \n",
       "63                 1               1       3.00               0   \n",
       "117                1               1       3.75               1   \n",
       "133                1               1       3.00               0   \n",
       "\n",
       "     interacted_1111  rfm_segment  \n",
       "6                  1          VIP  \n",
       "30                 1        Other  \n",
       "40                 1          VIP  \n",
       "63                 1  Oportunista  \n",
       "117                1        Other  \n",
       "133                1  Oportunista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIPCION NUMERICA RFM -----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recency_days</th>\n",
       "      <th>frequency_activity</th>\n",
       "      <th>frequency_purchases</th>\n",
       "      <th>monetary_proxy</th>\n",
       "      <th>RFM_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>260864.000</td>\n",
       "      <td>260864.000</td>\n",
       "      <td>260864.000</td>\n",
       "      <td>260864.000</td>\n",
       "      <td>260864.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.034</td>\n",
       "      <td>9.294</td>\n",
       "      <td>0.387</td>\n",
       "      <td>7.370</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>142.510</td>\n",
       "      <td>9.366</td>\n",
       "      <td>1.426</td>\n",
       "      <td>9.505</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>3.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000</td>\n",
       "      <td>37.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>625.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       recency_days  frequency_activity  frequency_purchases  monetary_proxy  \\\n",
       "count    260864.000          260864.000           260864.000      260864.000   \n",
       "mean          2.034               9.294                0.387           7.370   \n",
       "std         142.510               9.366                1.426           9.505   \n",
       "min          -1.000               0.000                0.000           0.000   \n",
       "25%           0.000               3.000                0.000           3.000   \n",
       "50%           0.000               6.000                0.000           4.000   \n",
       "75%           0.000              12.000                0.000           8.000   \n",
       "max        9999.000              37.000              107.000         625.000   \n",
       "\n",
       "        RFM_score  \n",
       "count  260864.000  \n",
       "mean        3.000  \n",
       "std         0.919  \n",
       "min         1.000  \n",
       "25%         2.250  \n",
       "50%         3.000  \n",
       "75%         3.500  \n",
       "max         5.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DISTRIBUCION SCORES (counts por score) -----\n",
      "\n",
      "recency_score:\n",
      " recency_score\n",
      "1    52173\n",
      "2    52173\n",
      "3    52173\n",
      "4    52173\n",
      "5    52172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "frequency_score:\n",
      " frequency_score\n",
      "1    52172\n",
      "2    52173\n",
      "3    52173\n",
      "4    52173\n",
      "5    52173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "purchases_score:\n",
      " purchases_score\n",
      "1    52172\n",
      "2    52173\n",
      "3    52173\n",
      "4    52173\n",
      "5    52173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "monetary_score:\n",
      " monetary_score\n",
      "1    52172\n",
      "2    52173\n",
      "3    52173\n",
      "4    52173\n",
      "5    52173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "----- Conteo por segmento RFM -----\n",
      "rfm_segment\n",
      "Oportunista    126693\n",
      "Other           70825\n",
      "Attentive       45455\n",
      "VIP             17891\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RFM features creadas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subconjunto: solo clientes nuevos\n",
    "df_rfm = train[train['label'].isin([0,1])].copy()\n",
    "\n",
    "# Asegurar fechas en formato datetime\n",
    "df_rfm['date_max'] = pd.to_datetime(df_rfm['date_max'], errors='coerce')\n",
    "\n",
    "# Referencia Double-11 (ajustable si tu definición difiere)\n",
    "reference_date = pd.to_datetime('2014-11-11')\n",
    "\n",
    "# Recency: dias desde la ultima interaccion hasta Double-11\n",
    "df_rfm['recency_days'] = (reference_date - df_rfm['date_max']).dt.days\n",
    "# Si hay NaT -> asignamos un valor grande(que no se vio recientemente)\n",
    "df_rfm['recency_days'] = df_rfm['recency_days'].fillna(9999).astype(int)\n",
    "\n",
    "# Frequency proxies\n",
    "df_rfm['frequency_activity'] = df_rfm['activity_len'].fillna(0).astype(int)\n",
    "df_rfm['frequency_purchases'] = df_rfm['actions_3'].fillna(0).astype(int)\n",
    "\n",
    "# Monetary proxy (simple suma de diversidades como indicador de \"gasto\"/interes)\n",
    "df_rfm['monetary_proxy'] = df_rfm[['unique_items','unique_categories','unique_brands']].fillna(0).sum(axis=1).astype(int)\n",
    "\n",
    "# Flags utiles\n",
    "df_rfm['multiple_dates'] = (df_rfm['day_span'].fillna(0) > 0).astype(int)\n",
    "df_rfm['interacted_1111'] = df_rfm.get('has_1111', 0).fillna(0).astype(int)\n",
    "\n",
    "# Funcion robusta para convertir a scores por quintil sin depender de qcut\n",
    "def quantile_score(series, bins=5, invert=False):\n",
    "    # rank para mantener orden; evita errores si hay muchos valores iguales\n",
    "    ranks = series.rank(method='first', na_option='bottom')\n",
    "    n = len(series)\n",
    "    # divisor floored; cuidamos que no sea 0\n",
    "    denom = max(n / bins, 1)\n",
    "    scores = np.ceil(ranks / denom).astype(int)\n",
    "    scores = np.clip(scores, 1, bins)\n",
    "    if invert:\n",
    "        scores = (bins + 1) - scores\n",
    "    return scores\n",
    "\n",
    "# Crear scores (1..5). Para recency invertimos (mas reciente -> mayor score)\n",
    "df_rfm['recency_score']   = quantile_score(df_rfm['recency_days'], bins=5, invert=True)\n",
    "df_rfm['frequency_score'] = quantile_score(df_rfm['frequency_activity'], bins=5, invert=False)\n",
    "df_rfm['purchases_score'] = quantile_score(df_rfm['frequency_purchases'], bins=5, invert=False)\n",
    "df_rfm['monetary_score']  = quantile_score(df_rfm['monetary_proxy'], bins=5, invert=False)\n",
    "\n",
    "# Componer un RFM agregado (pesos: recency 0.5, frequency 0.25, monetary 0.25)\n",
    "df_rfm['RFM_score'] = (\n",
    "    df_rfm['recency_score'] * 0.50 +\n",
    "    df_rfm['frequency_score'] * 0.25 +\n",
    "    df_rfm['monetary_score'] * 0.25\n",
    ")\n",
    "\n",
    "\n",
    "def rfm_segment(row):\n",
    "    if (row['RFM_score'] >= 4.5) and (row['multiple_dates'] == 1):\n",
    "        return 'VIP'\n",
    "    if (row['RFM_score'] >= 3.5) and (row['monetary_score'] >= 4):\n",
    "        return 'Attentive'\n",
    "    if (row['interacted_1111'] == 1) and (row['multiple_dates'] == 0):\n",
    "        return 'Oportunista'\n",
    "    return 'Other'\n",
    "\n",
    "df_rfm['rfm_segment'] = df_rfm.apply(rfm_segment, axis=1)\n",
    "\n",
    "\n",
    "cols_check = [\n",
    "    'user_id','label','date_max','recency_days',\n",
    "    'recency_score','frequency_score','purchases_score','monetary_score','RFM_score',\n",
    "    'multiple_dates','interacted_1111','rfm_segment'\n",
    "]\n",
    "\n",
    "print(\"----- HEAD  -----\")\n",
    "display(df_rfm[cols_check].head(6))\n",
    "\n",
    "print(\"\\n----- DESCRIPCION NUMERICA RFM -----\")\n",
    "display(df_rfm[['recency_days','frequency_activity','frequency_purchases','monetary_proxy','RFM_score']].describe().round(3))\n",
    "\n",
    "print(\"\\n----- DISTRIBUCION SCORES (counts por score) -----\")\n",
    "for c in ['recency_score','frequency_score','purchases_score','monetary_score']:\n",
    "    print(f\"\\n{c}:\\n\", df_rfm[c].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n----- Conteo por segmento RFM -----\")\n",
    "print(df_rfm['rfm_segment'].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nRFM features creadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f93263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X_train_rf: (208691, 11) X_val_rf: (52173, 11)\n",
      "\n",
      "RF PR-AUC (Average Precision): 0.074934\n",
      "RF ROC-AUC: 0.521989\n",
      "Mejor threshold RF por F1: 0.4995 -> F1: 0.1269\n",
      "Precision@best: 0.0851, Recall@best: 0.2498\n",
      "\n",
      "Classification report (RF, threshold óptimo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9441    0.8250    0.8806     48983\n",
      "           1     0.0851    0.2498    0.1269      3190\n",
      "\n",
      "    accuracy                         0.7899     52173\n",
      "   macro avg     0.5146    0.5374    0.5037     52173\n",
      "weighted avg     0.8916    0.7899    0.8345     52173\n",
      "\n",
      "\n",
      "Confusion matrix (RF, threshold óptimo):\n",
      "[[40412  8571]\n",
      " [ 2393   797]]\n",
      "\n",
      "Feature importances (RF):\n",
      "frequency_activity   -> 0.368464\n",
      "monetary_proxy       -> 0.290674\n",
      "frequency_purchases  -> 0.071212\n",
      "RFM_score            -> 0.057509\n",
      "purchases_score      -> 0.056575\n",
      "monetary_score       -> 0.050116\n",
      "multiple_dates       -> 0.039097\n",
      "recency_score        -> 0.035467\n",
      "frequency_score      -> 0.030784\n",
      "recency_days         -> 0.000071\n",
      "interacted_1111      -> 0.000030\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1) Features a usar (RFM-only, interpretables)\n",
    "rf_features = [\n",
    "    'recency_days', 'frequency_activity', 'frequency_purchases', 'monetary_proxy',\n",
    "    'recency_score', 'frequency_score', 'purchases_score', 'monetary_score',\n",
    "    'RFM_score', 'multiple_dates', 'interacted_1111'\n",
    "]\n",
    "\n",
    "# 2) Preparar X, y\n",
    "X_rfm = df_rfm[rf_features].copy()\n",
    "y_rfm = df_rfm['label'].astype(int)\n",
    "\n",
    "# 3) Split\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(\n",
    "    X_rfm, y_rfm, test_size=0.20, stratify=y_rfm, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Shapes -> X_train_rf:\", X_train_rf.shape, \"X_val_rf:\", X_val_rf.shape)\n",
    "\n",
    "# 4) Configurar Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5) Entrenar\n",
    "rf.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# 6) Prediccion probabilistica en validacion\n",
    "y_val_proba_rf = rf.predict_proba(X_val_rf)[:, 1]\n",
    "\n",
    "# 7) Metricas: PR-AUC y ROC-AUC\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, classification_report, confusion_matrix\n",
    "\n",
    "ap_rf = average_precision_score(y_val_rf, y_val_proba_rf)\n",
    "roc_rf = roc_auc_score(y_val_rf, y_val_proba_rf)\n",
    "print(f\"\\nRF PR-AUC (Average Precision): {ap_rf:.6f}\")\n",
    "print(f\"RF ROC-AUC: {roc_rf:.6f}\")\n",
    "\n",
    "# 8) Buscar umbral que maximice F1\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_rf, y_val_proba_rf)\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold_rf = thresholds[best_idx]\n",
    "best_f1_rf = f1_scores[best_idx]\n",
    "print(f\"Mejor threshold RF por F1: {best_threshold_rf:.4f} -> F1: {best_f1_rf:.4f}\")\n",
    "print(f\"Precision@best: {precisions[:-1][best_idx]:.4f}, Recall@best: {recalls[:-1][best_idx]:.4f}\")\n",
    "\n",
    "# 9) Evaluacion con ese umbral\n",
    "y_val_pred_rf = (y_val_proba_rf >= best_threshold_rf).astype(int)\n",
    "print(\"\\nClassification report (RF, threshold óptimo):\")\n",
    "print(classification_report(y_val_rf, y_val_pred_rf, digits=4))\n",
    "print(\"\\nConfusion matrix (RF, threshold óptimo):\")\n",
    "print(confusion_matrix(y_val_rf, y_val_pred_rf))\n",
    "\n",
    "# 10) Importancia de features\n",
    "importances = rf.feature_importances_\n",
    "feat_imp = sorted(zip(rf_features, importances), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature importances (RF):\")\n",
    "for name, imp in feat_imp:\n",
    "    print(f\"{name:20s} -> {imp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bc25c",
   "metadata": {},
   "source": [
    "## Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tabla Comparativa de Eficiencia (Métricas Clave) ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6d57e_row0_col0, #T_6d57e_row0_col1, #T_6d57e_row0_col2 {\n",
       "  font-weight: bold;\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6d57e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6d57e_level0_col0\" class=\"col_heading level0 col0\" >ROC-AUC (val)</th>\n",
       "      <th id=\"T_6d57e_level0_col1\" class=\"col_heading level0 col1\" >PR-AUC (Avg. Precision)</th>\n",
       "      <th id=\"T_6d57e_level0_col2\" class=\"col_heading level0 col2\" >Máx F1-Score (val)</th>\n",
       "      <th id=\"T_6d57e_level0_col3\" class=\"col_heading level0 col3\" >Recall @ Máx F1</th>\n",
       "      <th id=\"T_6d57e_level0_col4\" class=\"col_heading level0 col4\" >Precision @ Máx F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Modelo</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6d57e_level0_row0\" class=\"row_heading level0 row0\" >LightGBM</th>\n",
       "      <td id=\"T_6d57e_row0_col0\" class=\"data row0 col0\" >0.6497</td>\n",
       "      <td id=\"T_6d57e_row0_col1\" class=\"data row0 col1\" >0.1232</td>\n",
       "      <td id=\"T_6d57e_row0_col2\" class=\"data row0 col2\" >0.1875</td>\n",
       "      <td id=\"T_6d57e_row0_col3\" class=\"data row0 col3\" >0.2900</td>\n",
       "      <td id=\"T_6d57e_row0_col4\" class=\"data row0 col4\" >0.1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6d57e_level0_row1\" class=\"row_heading level0 row1\" >XGBoost</th>\n",
       "      <td id=\"T_6d57e_row1_col0\" class=\"data row1 col0\" >0.6426</td>\n",
       "      <td id=\"T_6d57e_row1_col1\" class=\"data row1 col1\" >0.1188</td>\n",
       "      <td id=\"T_6d57e_row1_col2\" class=\"data row1 col2\" >0.1832</td>\n",
       "      <td id=\"T_6d57e_row1_col3\" class=\"data row1 col3\" >0.2997</td>\n",
       "      <td id=\"T_6d57e_row1_col4\" class=\"data row1 col4\" >0.1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6d57e_level0_row2\" class=\"row_heading level0 row2\" >Random Forest (RFM)</th>\n",
       "      <td id=\"T_6d57e_row2_col0\" class=\"data row2 col0\" >0.5220</td>\n",
       "      <td id=\"T_6d57e_row2_col1\" class=\"data row2 col1\" >0.0749</td>\n",
       "      <td id=\"T_6d57e_row2_col2\" class=\"data row2 col2\" >0.1269</td>\n",
       "      <td id=\"T_6d57e_row2_col3\" class=\"data row2 col3\" >0.2498</td>\n",
       "      <td id=\"T_6d57e_row2_col4\" class=\"data row2 col4\" >0.0851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x71c0f35eac80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# XGBoost: Umbral optimo\n",
    "precisions_xgb, recalls_xgb, thresholds_xgb = precision_recall_curve(y_val, y_val_proba)\n",
    "f1_scores_xgb = 2 * (precisions_xgb[:-1] * recalls_xgb[:-1]) / (precisions_xgb[:-1] + recalls_xgb[:-1] + 1e-12)\n",
    "best_idx_xgb = f1_scores_xgb.argmax()\n",
    "best_f1_xgb = f1_scores_xgb[best_idx_xgb]\n",
    "ap_xgb = average_precision_score(y_val, y_val_proba)\n",
    "roc_xgb = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "# LightGBM\n",
    "precisions_lgb, recalls_lgb, thresholds_lgb = precision_recall_curve(y_val_lgb, y_val_proba_lgb)\n",
    "f1_scores_lgb = 2 * (precisions_lgb[:-1] * recalls_lgb[:-1]) / (precisions_lgb[:-1] + recalls_lgb[:-1] + 1e-12)\n",
    "best_idx_lgb = f1_scores_lgb.argmax()\n",
    "best_f1_lgb = f1_scores_lgb[best_idx_lgb]\n",
    "ap_lgb = average_precision_score(y_val_lgb, y_val_proba_lgb)\n",
    "\n",
    "# Random Forest\n",
    "precisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_val_rf, y_val_proba_rf)\n",
    "f1_scores_rf = 2 * (precisions_rf[:-1] * recalls_rf[:-1]) / (precisions_rf[:-1] + recalls_rf[:-1] + 1e-12)\n",
    "best_idx_rf = f1_scores_rf.argmax()\n",
    "best_f1_rf = f1_scores_rf[best_idx_rf]\n",
    "ap_rf = average_precision_score(y_val_rf, y_val_proba_rf)\n",
    "\n",
    "\n",
    "# --- DataFrame Comparativo ---\n",
    "\n",
    "data = {\n",
    "    'Modelo': ['XGBoost', 'LightGBM', 'Random Forest (RFM)'],\n",
    "    'ROC-AUC (val)': [roc_xgb, roc_auc_score(y_val_lgb, y_val_proba_lgb), roc_auc_score(y_val_rf, y_val_proba_rf)],\n",
    "    'PR-AUC (Avg. Precision)': [ap_xgb, ap_lgb, ap_rf],\n",
    "    'Máx F1-Score (val)': [best_f1_xgb, best_f1_lgb, best_f1_rf],\n",
    "    'Recall @ Máx F1': [recalls_xgb[:-1][best_idx_xgb], recalls_lgb[:-1][best_idx_lgb], recalls_rf[:-1][best_idx_rf]],\n",
    "    'Precision @ Máx F1': [precisions_xgb[:-1][best_idx_xgb], precisions_lgb[:-1][best_idx_lgb], precisions_rf[:-1][best_idx_rf]],\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data).set_index('Modelo').sort_values(by='PR-AUC (Avg. Precision)', ascending=False)\n",
    "\n",
    "# --- Impresión de Resultados ---\n",
    "\n",
    "print(\"--- Tabla Comparativa de Eficiencia (Métricas Clave) ---\\n\")\n",
    "display(df_metrics.style.highlight_max(axis=0, subset=['ROC-AUC (val)', 'PR-AUC (Avg. Precision)', 'Máx F1-Score (val)'], props='font-weight: bold; background-color: lightgreen;').format('{:.4f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c217205",
   "metadata": {},
   "source": [
    "### Evaluación por Modelo\n",
    "* LightGBM (Modelo Más Eficiente)\n",
    "\n",
    "Eficiencia Superior: LightGBM supera a los demás modelos en las tres métricas principales (PR-AUC, Máx F1-Score y ROC-AUC).\n",
    "\n",
    "F1-Score (0.1875): Alcanza el mejor equilibrio, demostrando una ligera ventaja sobre XGBoost.\n",
    "\n",
    "Trade-off: Logra una Precision de 0.1386 y un Recall de 0.2900 en el punto de corte óptimo (máximo F1), lo que significa que de todos los casos que predice como positivos, solo cerca del 14% son correctos, pero logra capturar al 29% de los verdaderos positivos.\n",
    "\n",
    "* XGBoost (Rendimiento Fuerte)\n",
    "\n",
    "Muy Competitivo: XGBoost es muy similar a LightGBM en cuanto a la capacidad de discriminación general (ROC-AUC de 0.6426).\n",
    "\n",
    "Recall Ligeramente Mejor (0.2997): Alcanzó un recall marginalmente superior a LightGBM en su punto de F1 máximo, lo que indica que logró identificar a un pequeño porcentaje más de casos positivos reales, aunque a costa de una precision un poco más baja.\n",
    "\n",
    "* Random Forest (RFM) (Rendimiento Bajo)\n",
    "\n",
    "Poco Efectivo: El modelo basado solo en las características RFM (Recency, Frequency, Monetary) tuvo un rendimiento significativamente inferior en todas las métricas.\n",
    "\n",
    "PR-AUC (0.0749): Esto sugiere que las características RFM puras no contienen suficiente información predictiva para la clase minoritaria, o que el modelo Random Forest no es tan potente como los Gradient Boosting Machines para este problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
